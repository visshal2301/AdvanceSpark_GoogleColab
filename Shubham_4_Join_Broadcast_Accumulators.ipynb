{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFzPjRDFUvhQ5L3m6ItayU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visshal2301/AdvanceSpark_GoogleColab/blob/main/Shubham_4_Join_Broadcast_Accumulators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drHn1Ejxm297",
        "outputId": "41b0be42-e934-40ce-e7cd-9b0d84c92f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|val1|\n",
            "+---+----+\n",
            "|  1|   A|\n",
            "|  2|   B|\n",
            "|  3|   C|\n",
            "+---+----+\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [id#10L, val1#9]\n",
            "   +- BroadcastHashJoin [id#10L], [id#8L], Inner, BuildRight, false\n",
            "      :- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=160]\n",
            "         +- Filter isnotnull(id#8L)\n",
            "            +- Scan ExistingRDD[id#8L,val1#9]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#BroadCast\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
        "\n",
        "# Small dataset (can be broadcasted)\n",
        "small_df = spark.createDataFrame(\n",
        "    [(1, \"A\"), (2, \"B\"), (3, \"C\")],\n",
        "    [\"id\", \"val1\"]\n",
        ")\n",
        "\n",
        "# Large dataset\n",
        "large_df = spark.range(0, 1000000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Force Broadcast Hash Join\n",
        "broadcast_join = large_df.join(small_df.hint(\"BROADCAST\"), \"id\", \"inner\")\n",
        "\n",
        "broadcast_join.show(5)\n",
        "broadcast_join.explain()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are absolutely right! The explain() output clearly shows that Spark applied a BroadcastHashJoin.\n",
        "\n",
        "Specifically, you can see BroadcastHashJoin listed in the physical plan, and below it, BroadcastExchange indicates that the small_df (the right side of the join, BuildRight) was broadcasted to all executor nodes. This confirms the broadcast strategy was successfully utilized as intended by the hint(\"BROADCAST\")."
      ],
      "metadata": {
        "id": "Pmu0E_odo1uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Default value: 10 MB (in most Spark versions).\n",
        "- can be configured to 8 gb . but it is very unlikely that process will happen\n",
        "\n",
        "- Meaning: If the size of a DataFrame/table is estimated to be less than this threshold, Spark will broadcast it to all worker nodes and perform a Broadcast Hash Join.\n",
        "- Effect: This avoids shuffling the larger dataset, making joins much faster for small lookup tables.\n"
      ],
      "metadata": {
        "id": "rBoeUtinrkWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxfquLTnrtXz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#broadcast Hash Join\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
        "\n",
        "# Medium-sized dataset (too large to broadcast efficiently)\n",
        "medium_df = spark.range(0, 500000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Another large dataset\n",
        "large_df2 = spark.range(0, 1000000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Force Shuffle Hash Join\n",
        "shuffle_hash_join = medium_df.hint(\"SHUFFLE_HASH\").join(large_df2, \"id\", \"inner\")\n",
        "\n",
        "# The original intention might have been to show some rows and then explain the plan.\n",
        "# If you intended to save the DataFrame, you need to specify a format and a path, e.g.:\n",
        "# shuffle_hash_join.write.format(\"parquet\").mode(\"overwrite\").save(\"output_path.parquet\")\n",
        "\n",
        "# Showing the top 5 rows\n",
        "shuffle_hash_join.show(5)\n",
        "\n",
        "# Explaining the physical plan of the join\n",
        "shuffle_hash_join.explain()\n",
        "\n",
        "# To simulate a write without actually writing the data, call .explain() on the DataFrameWriter\n",
        "# This shows the plan for the write operation.\n",
        "# The DataFrameWriter object itself does not have an explain method.\n",
        "# The explain() on the DataFrame itself (as shown above) usually provides the relevant plan.\n",
        "# print(\"\\n--- Explaining the write operation (simulation only) ---\")\n",
        "# shuffle_hash_join.write.format(\"parquet\").mode(\"overwrite\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3qEDue8pA75",
        "outputId": "7050ed9b-abed-4660-ed23-e6fc5d8cd5e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|  26|\n",
            "|  29|\n",
            "| 474|\n",
            "| 964|\n",
            "|1677|\n",
            "+----+\n",
            "only showing top 5 rows\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [id#19L]\n",
            "   +- ShuffledHashJoin [id#19L], [id#21L], Inner, BuildLeft\n",
            "      :- Exchange hashpartitioning(id#19L, 200), ENSURE_REQUIREMENTS, [plan_id=262]\n",
            "      :  +- Range (0, 500000, step=1, splits=2)\n",
            "      +- Exchange hashpartitioning(id#21L, 200), ENSURE_REQUIREMENTS, [plan_id=263]\n",
            "         +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of explain() confirms that a ShuffledHashJoin was applied, as you explicitly hinted for with medium_df.hint(\"SHUFFLE_HASH\").\n",
        "\n",
        "Key elements in the physical plan that confirm this are:\n",
        "\n",
        "ShuffledHashJoin: This directly indicates the join strategy used.\n",
        "Exchange hashpartitioning: You'll see this operation for both datasets (medium_df and large_df2). This signifies that both DataFrames were re-partitioned (shuffled) across the network based on their join keys (id#33L and id#35L). This shuffling is necessary for a shuffle hash join to ensure that all rows with the same join key are brought together on the same executor to perform the hash join locally."
      ],
      "metadata": {
        "id": "2ZtsRRpLplQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Both datasets are shuffled across the cluster by id.\n",
        "- In each partition, Spark builds a hash table from the smaller side (medium_df relative to large_df2).\n",
        "- The larger dataset probes the hash table for matches.\n",
        "- Broadcast is avoided because medium_df is too large to efficiently send to every executor.\n"
      ],
      "metadata": {
        "id": "5eI_o17zpyi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Broadcast Hash Join: Best when one dataset is very small (few MBs).\n",
        "- Shuffle Hash Join: Used when broadcast is infeasible but hashing is still efficient\n"
      ],
      "metadata": {
        "id": "oAH-p6GYq1N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ShuffleHashJoinExample\").getOrCreate()\n",
        "\n",
        "# Two medium-sized DataFrames (too large to broadcast efficiently)\n",
        "df1 = spark.range(0, 500000).withColumnRenamed(\"id\", \"id\")\n",
        "df2 = spark.range(250000, 750000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Force Shuffle Hash Join using a hint\n",
        "joined_df = df1.hint(\"SHUFFLE_HASH\").join(df2, \"id\", \"inner\")\n",
        "\n",
        "joined_df.show(5)\n",
        "shuffle_hash_join.explain()"
      ],
      "metadata": {
        "id": "ZjLnKTOAqw7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8703db95-eaa6-443d-fb5d-91bef5f6001e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|    id|\n",
            "+------+\n",
            "|250267|\n",
            "|250622|\n",
            "|250722|\n",
            "|250780|\n",
            "|250953|\n",
            "+------+\n",
            "only showing top 5 rows\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [id#19L]\n",
            "   +- ShuffledHashJoin [id#19L], [id#21L], Inner, BuildLeft\n",
            "      :- Exchange hashpartitioning(id#19L, 200), ENSURE_REQUIREMENTS, [plan_id=262]\n",
            "      :  +- Range (0, 500000, step=1, splits=2)\n",
            "      +- Exchange hashpartitioning(id#21L, 200), ENSURE_REQUIREMENTS, [plan_id=263]\n",
            "         +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This physical plan confirms that a ShuffledHashJoin was performed, as intended by the hint. Here's a breakdown:\n",
        "\n",
        "ShuffledHashJoin [id#19L], [id#21L], Inner, BuildLeft: This is the core operation. It means Spark is using a Hash Join strategy where data is first shuffled.\n",
        "\n",
        "[id#19L], [id#21L]: These are the join keys from the two DataFrames (df1 and df2 in your example).\n",
        "Inner: Specifies the type of join.\n",
        "BuildLeft: Indicates that Spark built a hash table using the left DataFrame (df1, the smaller one after shuffling) in each partition.\n",
        "Exchange hashpartitioning(id#19L, 200), ENSURE_REQUIREMENTS (for both datasets): This is the crucial part that signifies shuffling.\n",
        "\n",
        "Exchange hashpartitioning: Both DataFrames are re-partitioned across the cluster based on the hash of their id column. This ensures that rows with the same id from both DataFrames end up on the same executor to be joined locally.\n",
        "200: Refers to the default number of partitions Spark creates during shuffling.\n",
        "In essence, Spark moved data across the network (shuffled) to group matching keys together, and then performed a hash join within each partition."
      ],
      "metadata": {
        "id": "hfAxzW5sumSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bucketing\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataBucketingJoin\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Simulate large datasets\n",
        "trades_df = spark.range(0, 10_000_000).withColumnRenamed(\"id\", \"trade_id\")\n",
        "orders_df = spark.range(5_000_000, 15_000_000).withColumnRenamed(\"id\", \"order_id\")\n",
        "\n",
        "# Write bucketed tables (same column and same number of buckets)\n",
        "trades_df.write \\\n",
        "    .bucketBy(16, \"trade_id\") \\\n",
        "    .sortBy(\"trade_id\") \\\n",
        "    .saveAsTable(\"trades_bucketed\")\n",
        "\n",
        "orders_df.write \\\n",
        "    .bucketBy(16, \"order_id\") \\\n",
        "    .sortBy(\"order_id\") \\\n",
        "    .saveAsTable(\"orders_bucketed\")\n",
        "\n",
        "# Perform join on bucketed tables\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT t.trade_id, o.order_id\n",
        "    FROM trades_bucketed t\n",
        "    JOIN orders_bucketed o\n",
        "    ON t.trade_id = o.order_id\n",
        "\"\"\")\n",
        "\n",
        "result.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQh79ZyUulvs",
        "outputId": "2a3cb166-3da4-44ce-eb92-7d0fd38fe1b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|trade_id|order_id|\n",
            "+--------+--------+\n",
            "| 5000017| 5000017|\n",
            "| 5000045| 5000045|\n",
            "| 5000057| 5000057|\n",
            "| 5000095| 5000095|\n",
            "| 5000102| 5000102|\n",
            "| 5000105| 5000105|\n",
            "| 5000107| 5000107|\n",
            "| 5000132| 5000132|\n",
            "| 5000151| 5000151|\n",
            "| 5000166| 5000166|\n",
            "+--------+--------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f371ef56"
      },
      "source": [
        "# Task\n",
        "Measure and compare the execution times of Broadcast Hash Join, Shuffle Hash Join, and Bucketed Join in Spark, then summarize their performance differences and suitable use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f183f6d6"
      },
      "source": [
        "## Measure Broadcast Join Execution Time\n",
        "\n",
        "### Subtask:\n",
        "Modify the Broadcast Hash Join code (from cell drHn1Ejxm297) to include timing measurements for its execution, printing the duration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595246eb"
      },
      "source": [
        "**Reasoning**:\n",
        "To measure the execution time of the Broadcast Hash Join, I will modify the existing code cell `drHn1Ejxm297` by importing the `time` module, adding `time.time()` calls before and after the join operation and display, and then calculating and printing the duration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69077875",
        "outputId": "78d0cb88-e009-468e-839c-483fbcc226fd"
      },
      "source": [
        "import time # Import the time module\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
        "\n",
        "# Small dataset (can be broadcasted)\n",
        "small_df = spark.createDataFrame(\n",
        "    [(1, \"A\"), (2, \"B\"), (3, \"C\")],\n",
        "    [\"id\", \"val1\"]\n",
        ")\n",
        "\n",
        "# Large dataset\n",
        "large_df = spark.range(0, 1000000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Record start time for Broadcast Hash Join\n",
        "start_time_broadcast = time.time()\n",
        "\n",
        "# Force Broadcast Hash Join\n",
        "broadcast_join = large_df.join(small_df.hint(\"BROADCAST\"), \"id\", \"inner\")\n",
        "\n",
        "broadcast_join.show(5)\n",
        "broadcast_join.explain()\n",
        "\n",
        "# Record end time for Broadcast Hash Join\n",
        "end_time_broadcast = time.time()\n",
        "\n",
        "# Calculate and print duration\n",
        "duration_broadcast = end_time_broadcast - start_time_broadcast\n",
        "print(f\"Broadcast Hash Join took {duration_broadcast:.4f} seconds\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|val1|\n",
            "+---+----+\n",
            "|  1|   A|\n",
            "|  2|   B|\n",
            "|  3|   C|\n",
            "+---+----+\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [id#52L, val1#51]\n",
            "   +- BroadcastHashJoin [id#52L], [id#50L], Inner, BuildRight, false\n",
            "      :- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=540]\n",
            "         +- Filter isnotnull(id#50L)\n",
            "            +- Scan ExistingRDD[id#50L,val1#51]\n",
            "\n",
            "\n",
            "Broadcast Hash Join took 0.9236 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abe9f328"
      },
      "source": [
        "## Measure Shuffle Hash Join Execution Time\n",
        "\n",
        "### Subtask:\n",
        "Modify the Shuffle Hash Join code (from cell x3qEDue8pA75) to include timing measurements for its execution, printing the duration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405cab02"
      },
      "source": [
        "**Reasoning**:\n",
        "To measure the execution time of the Shuffle Hash Join, I will modify the existing code cell `x3qEDue8pA75` by importing the `time` module, adding `time.time()` calls before and after the join operation and `explain()` call, and then calculating and printing the duration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d393dac8",
        "outputId": "e912bab8-9dfb-4225-f91f-cdc02826b600"
      },
      "source": [
        "import time # Import the time module\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
        "\n",
        "# Medium-sized dataset (too large to broadcast efficiently)\n",
        "medium_df = spark.range(0, 500000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Another large dataset\n",
        "large_df2 = spark.range(0, 1000000).withColumnRenamed(\"id\", \"id\")\n",
        "\n",
        "# Record start time for Shuffle Hash Join\n",
        "start_time_shuffle = time.time()\n",
        "\n",
        "# Force Shuffle Hash Join\n",
        "shuffle_hash_join = medium_df.hint(\"SHUFFLE_HASH\").join(large_df2, \"id\", \"inner\")\n",
        "\n",
        "# Showing the top 5 rows\n",
        "shuffle_hash_join.show(5)\n",
        "\n",
        "# Explaining the physical plan of the join\n",
        "shuffle_hash_join.explain()\n",
        "\n",
        "# Record end time for Shuffle Hash Join\n",
        "end_time_shuffle = time.time()\n",
        "\n",
        "# Calculate and print duration\n",
        "duration_shuffle = end_time_shuffle - start_time_shuffle\n",
        "print(f\"Shuffle Hash Join took {duration_shuffle:.4f} seconds\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|  id|\n",
            "+----+\n",
            "|  26|\n",
            "|  29|\n",
            "| 474|\n",
            "| 964|\n",
            "|1677|\n",
            "+----+\n",
            "only showing top 5 rows\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [id#61L]\n",
            "   +- ShuffledHashJoin [id#61L], [id#63L], Inner, BuildLeft\n",
            "      :- Exchange hashpartitioning(id#61L, 200), ENSURE_REQUIREMENTS, [plan_id=642]\n",
            "      :  +- Range (0, 500000, step=1, splits=2)\n",
            "      +- Exchange hashpartitioning(id#63L, 200), ENSURE_REQUIREMENTS, [plan_id=643]\n",
            "         +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n",
            "Shuffle Hash Join took 2.1695 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af17ba90"
      },
      "source": [
        "## Measure Bucketed Join Execution Time\n",
        "\n",
        "### Subtask:\n",
        "Modify the Bucketed Join code (from cell rQh79ZyUulvs) to include timing measurements for its execution, printing the duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e9806d4"
      },
      "source": [
        "#### Reasoning:\n",
        "To measure the execution time of the Bucketed Join, I will modify the existing code cell `rQh79ZyUulvs` by importing the `time` module, adding `time.time()` calls before and after the join operation and display, and then calculating and printing the duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2f4eae9"
      },
      "source": [
        "**Reasoning**:\n",
        "To measure the execution time of the Bucketed Join, I will modify the existing code by importing the `time` module, placing `time.time()` calls around the Spark SQL query execution and `show()` action, and then calculating and printing the duration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8028124",
        "outputId": "0aec9d85-ea1d-4ee3-b821-8b14e68f80e4"
      },
      "source": [
        "import time # Import the time module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataBucketingJoin\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Simulate large datasets\n",
        "trades_df = spark.range(0, 10_000_000).withColumnRenamed(\"id\", \"trade_id\")\n",
        "orders_df = spark.range(5_000_000, 15_000_000).withColumnRenamed(\"id\", \"order_id\")\n",
        "\n",
        "# Write bucketed tables (same column and same number of buckets)\n",
        "# These operations are usually done once. If tables already exist, this will overwrite.\n",
        "trades_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .bucketBy(16, \"trade_id\") \\\n",
        "    .sortBy(\"trade_id\") \\\n",
        "    .saveAsTable(\"trades_bucketed\")\n",
        "\n",
        "orders_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .bucketBy(16, \"order_id\") \\\n",
        "    .sortBy(\"order_id\") \\\n",
        "    .saveAsTable(\"orders_bucketed\")\n",
        "\n",
        "# Record start time for Bucketed Join\n",
        "start_time_bucketed = time.time()\n",
        "\n",
        "# Perform join on bucketed tables\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT t.trade_id, o.order_id\n",
        "    FROM trades_bucketed t\n",
        "    JOIN orders_bucketed o\n",
        "    ON t.trade_id = o.order_id\n",
        "\"\"\")\n",
        "\n",
        "result.show(10)\n",
        "\n",
        "# Record end time for Bucketed Join\n",
        "end_time_bucketed = time.time()\n",
        "\n",
        "# Calculate and print duration\n",
        "duration_bucketed = end_time_bucketed - start_time_bucketed\n",
        "print(f\"Bucketed Join took {duration_bucketed:.4f} seconds\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|trade_id|order_id|\n",
            "+--------+--------+\n",
            "| 5000017| 5000017|\n",
            "| 5000045| 5000045|\n",
            "| 5000057| 5000057|\n",
            "| 5000095| 5000095|\n",
            "| 5000102| 5000102|\n",
            "| 5000105| 5000105|\n",
            "| 5000107| 5000107|\n",
            "| 5000132| 5000132|\n",
            "| 5000151| 5000151|\n",
            "| 5000166| 5000166|\n",
            "+--------+--------+\n",
            "only showing top 10 rows\n",
            "Bucketed Join took 0.7542 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9ef650"
      },
      "source": [
        "## Performance Comparison and Use Cases\n",
        "\n",
        "We have measured the execution times for Broadcast Hash Join, Shuffle Hash Join, and Bucketed Join:\n",
        "\n",
        "- **Broadcast Hash Join**: {duration_broadcast:.4f} seconds\n",
        "- **Shuffle Hash Join**: {duration_shuffle:.4f} seconds\n",
        "- **Bucketed Join**: {duration_bucketed:.4f} seconds\n",
        "\n",
        "### Summary of Performance Differences and Use Cases:\n",
        "\n",
        "1.  **Broadcast Hash Join**:\n",
        "    *   **Performance**: Often the fastest when one DataFrame is significantly small enough to fit into memory on all executor nodes.\n",
        "    *   **Reason**: It avoids data shuffling for the larger dataset, as the smaller dataset is broadcasted once and then used locally on each executor.\n",
        "    *   **Suitable Use Cases**: Lookup tables, dimension tables, or any scenario where one side of the join is relatively small (typically under 10MB or the `spark.sql.autoBroadcastJoinThreshold` limit).\n",
        "\n",
        "2.  **Shuffle Hash Join**:\n",
        "    *   **Performance**: Slower than Broadcast Join for small datasets but generally faster than Sort-Merge Join for medium-sized datasets, or when the `spark.sql.autoBroadcastJoinThreshold` is exceeded but the smaller of the two shuffled datasets can still fit in memory within each partition.\n",
        "    *   **Reason**: Requires shuffling both datasets based on the join key to bring matching keys together on the same partitions. After shuffling, a hash table is built from the smaller partitioned dataset in each partition.\n",
        "    *   **Suitable Use Cases**: When neither dataset is small enough to be broadcasted, but one dataset is still considerably smaller than the other, allowing for efficient hash table construction within each shuffled partition.\n",
        "\n",
        "3.  **Bucketed Join**:\n",
        "    *   **Performance**: Can be extremely fast, often comparable to or even better than Broadcast Join, especially for large datasets. It is the most performant join strategy when both datasets are already bucketed and sorted on the join key with the same number of buckets.\n",
        "    *   **Reason**: No shuffling or sorting is required at join time. Spark knows that rows with the same join key are in the same bucket number across both tables, allowing it to join only corresponding buckets without moving data across the network.\n",
        "    *   **Suitable Use Cases**: ETL pipelines where data is frequently joined, data warehousing scenarios, and recurring joins on very large datasets where pre-bucketing and sorting can be done once and reused multiple times.\n",
        "\n",
        "**Conclusion**:\n",
        "\n",
        "The choice of join strategy heavily depends on the characteristics and size of the datasets involved. For our specific examples:\n",
        "\n",
        "*   **Broadcast Hash Join** was very efficient when one table was tiny.\n",
        "*   **Shuffle Hash Join** was slower due to the overhead of shuffling larger datasets.\n",
        "*   **Bucketed Join** demonstrated strong performance, especially considering the larger scale of data involved, thanks to the pre-optimization of bucketing and sorting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b19c7b62"
      },
      "source": [
        "## Analyze Join Performance\n",
        "\n",
        "### Subtask:\n",
        "Summarize and compare the observed execution times for the Broadcast Hash Join, Shuffle Hash Join, and Bucketed Join, highlighting their performance differences and suitable use cases based on the timing results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06fc3cab"
      },
      "source": [
        "## Join Performance Summary and Comparison\n",
        "\n",
        "We have measured the execution times for three different Spark join strategies:\n",
        "\n",
        "- **Broadcast Hash Join**: `0.9236` seconds\n",
        "- **Shuffle Hash Join**: `2.1695` seconds\n",
        "- **Bucketed Join**: `0.7542` seconds\n",
        "\n",
        "### Performance Comparison:\n",
        "\n",
        "Based on the observed execution times, the ranking from fastest to slowest is:\n",
        "\n",
        "1.  **Bucketed Join** (0.7542 seconds)\n",
        "2.  **Broadcast Hash Join** (0.9236 seconds)\n",
        "3.  **Shuffle Hash Join** (2.1695 seconds)\n",
        "\n",
        "#### Detailed Analysis:\n",
        "\n",
        "*   **Bucketed Join**: This was the fastest join. Its superior performance comes from the fact that the data was already pre-partitioned and sorted (or at least bucketed by the join key) on disk. When joining two bucketed tables with the same number of buckets and bucket key, Spark can avoid the expensive shuffle phase entirely. Instead, it directly joins corresponding buckets on each executor, significantly reducing network I/O and computation. This pre-optimization is highly effective for frequently joined tables.\n",
        "\n",
        "*   **Broadcast Hash Join**: This was the second fastest. It performs well when one of the DataFrames is small enough to fit into the memory of all executor nodes. By broadcasting the smaller DataFrame, Spark avoids shuffling the larger DataFrame. All executors have a local copy of the small DataFrame, allowing them to perform the join locally with their partitions of the large DataFrame. The overhead comes from transferring the small DataFrame to all executors and building the hash table.\n",
        "\n",
        "*   **Shuffle Hash Join**: This was the slowest of the three in this specific scenario. It requires shuffling both DataFrames across the network based on the join key. This means all rows with the same key from both DataFrames are sent to the same executor. The shuffling process (network I/O, serialization/deserialization) is the most expensive part of this join strategy. After shuffling, a hash join is performed within each partition. This strategy is necessary when neither dataset can be broadcast (i.e., both are large).\n",
        "\n",
        "### Suitable Use Cases:\n",
        "\n",
        "*   **Broadcast Hash Join**:\n",
        "    *   **Dataset Size**: Ideal when one dataset is very small (typically below `spark.sql.autoBroadcastJoinThreshold`, default 10MB or configurable up to a few GBs).\n",
        "    *   **Frequency**: Suitable for ad-hoc queries or when the small table is a lookup table that frequently joins with larger fact tables.\n",
        "    *   **Data Characteristics**: When the small table is static or changes infrequently.\n",
        "    *   **Benefit**: Minimizes data shuffling, making it very efficient for appropriate data sizes.\n",
        "\n",
        "*   **Shuffle Hash Join**:\n",
        "    *   **Dataset Size**: When both datasets are large and cannot fit into executor memory for broadcasting. It's often chosen by Spark if `Broadcast Hash Join` is not possible and `Sort Merge Join` is less optimal.\n",
        "    *   **Frequency**: Used for general-purpose joins where data distribution is not pre-optimized.\n",
        "    *   **Data Characteristics**: Any type of data, but performance can be impacted by data skew (uneven distribution of keys).\n",
        "    *   **Benefit**: A robust general-purpose join for large datasets, but incurs significant shuffle overhead.\n",
        "\n",
        "*   **Bucketed Join**:\n",
        "    *   **Dataset Size**: Highly effective for very large datasets that are frequently joined, especially when those datasets are persistently stored (e.g., in Hive tables).\n",
        "    *   **Frequency**: Best for recurring ETL jobs, analytical queries, or applications where the same large tables are joined repeatedly.\n",
        "    *   **Data Characteristics**: Requires data to be bucketed (and optionally sorted) on the join key during storage. The bucket count and bucket key must match between the joined tables.\n",
        "    *   **Benefit**: Eliminates shuffling entirely for matching bucketed tables, leading to the fastest join performance for appropriately prepared large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e492757"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings regarding the execution times of different Spark join strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c14802f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**What are the performance differences between Broadcast Hash Join, Shuffle Hash Join, and Bucketed Join?**\n",
        "Based on the execution times observed, the Bucketed Join was the fastest at 0.7542 seconds, followed by the Broadcast Hash Join at 0.9236 seconds. The Shuffle Hash Join was the slowest, taking 2.1695 seconds.\n",
        "\n",
        "**What are the suitable use cases for each join strategy?**\n",
        "*   **Broadcast Hash Join**: Best when one dataset is small enough to fit into executor memory (typically under `spark.sql.autoBroadcastJoinThreshold`, default 10MB), ideal for lookup tables.\n",
        "*   **Shuffle Hash Join**: Suitable when both datasets are large and cannot be broadcasted, serving as a general-purpose join but incurring significant data shuffling overhead.\n",
        "*   **Bucketed Join**: Highly effective for very large datasets that are frequently joined and are pre-bucketed and optionally sorted on the join key with matching bucket configurations, as it avoids any shuffling at join time.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Bucketed Join** was the fastest strategy, completing in 0.7542 seconds, demonstrating superior performance when datasets are pre-optimized through bucketing.\n",
        "*   **Broadcast Hash Join** was the second fastest, executing in 0.9236 seconds, proving efficient for scenarios where one dataset is significantly small.\n",
        "*   **Shuffle Hash Join** was the slowest among the three, taking 2.1695 seconds, primarily due to the overhead of data shuffling required for larger, non-broadcastable datasets.\n",
        "*   The observed performance ranking from fastest to slowest was: Bucketed Join, Broadcast Hash Join, and Shuffle Hash Join.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   For optimal Spark SQL join performance, prioritize Bucketed Join when dealing with recurring joins on very large, persistent datasets that can be pre-processed, and Broadcast Hash Join for scenarios involving small lookup tables.\n",
        "*   When neither pre-bucketing nor broadcasting is feasible, Shuffle Hash Join is the default choice, but its performance should be carefully monitored, especially with data skew, to identify potential bottlenecks.\n"
      ]
    }
  ]
}